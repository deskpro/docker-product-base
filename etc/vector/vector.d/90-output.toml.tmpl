[transforms.all]
type = "remap"
inputs = [
  "process_supervisord",
  "process_php_fpm",
  "process_nginx",

  "nginx_access",
  "nginx_error",
  "php_error",
  "php_fpm_error",

  "deskpro_logs",
  "deskpro_services",

  "newrelic_php_agent",
  "newrelic_daemon"
]
source = '''

if is_string(.message) && contains(string!(.message), get_env_var!("VECTOR_MARKER")) {
  abort
}

if is_nullish(.lvl) {
  .lvl = string(.parsed.lvl) ?? string(.parsed.level) ?? string(.parsed.level_name) ?? string(.parsed.severity) ?? "INFO"
}

if is_nullish(.ts) {
  if is_timestamp(.parsed.ts) {
    .ts = .parsed.ts
  } else if is_timestamp(.parsed.timestamp) {
    .ts = .parsed.timestamp
  } else {
    .ts = now()
  }
}

.lvl = upcase(.lvl) ?? .lvl

.app = string(.app) ?? null
if is_nullish(.app) {
  .app = string(.source) ?? "unknown"
}

.chan = string(.chan) ?? null
if is_nullish(.chan) {
  .chan = "general"
}

.log_group = string(.log_group) ?? null
if is_nullish(.log_group) {
  .log_group = .app + "-" + .chan
}

.container_name = "{{ getenv "CONTAINER_NAME" }}"

data = {
  "ts": format_timestamp(.ts, format: "%Y-%m-%dT%H:%M:%SZ") ?? .ts,
  "ts_read": format_timestamp(now(), format: "%Y-%m-%dT%H:%M:%SZ") ?? null,
  "lvl": .lvl,
  "container_name": .container_name,
  "log_group": .log_group,
  "app": .app,
  "chan": .chan,
  "msg": string(.parsed.msg) ?? string(.parsed.message) ?? string(.message) ?? ""
}

if is_string(data.message) && contains(string!(data.message), get_env_var!("VECTOR_MARKER")) {
  abort
}

extra = {}

if is_object(.parsed) {
  del(.parsed.app)
  del(.parsed.chan)
  del(.parsed.msg)
  del(.parsed.message)
  del(.parsed.timestamp)
  del(.parsed.ts)
  del(.parsed.lvl)
  del(.parsed.level)
  del(.parsed.level_name)
  del(.parsed.severity)

  for_each(object!(.parsed)) -> |key, value| {
    extra = set(extra, [key], value) ?? extra
  }

  del(.parsed)
}

. = compact(merge(data, extra))
'''

{{if eq "dir" (getenv "LOGS_EXPORT_TARGET") }}
  {{if eq (getenv "LOGS_EXPORT_DIR") "/dev/null" }}
    # LOGS_EXPORT_DIR=/dev/null
    # So we wont enable any sinks.out_file
  {{else}}
    [sinks.out_file]
    type = "file"
    inputs = ["all"]
    path = "{{getenv "LOGS_EXPORT_DIR"}}/{{ getenv "LOGS_EXPORT_FILENAME" "{{.log_group}}.log" }}"
    encoding.codec = "json"
    encoding.timestamp_format = "rfc3339"
  {{end}}
{{else if eq "cloudwatch" (getenv "LOGS_EXPORT_TARGET") }}
[sinks.out_cloudwatch_logs]
type = "aws_cloudwatch_logs"
inputs = ["all"]
group_name = "{{ getenv "LOGS_EXPORT_CLOUDWATCH_GROUP_NAME" "deskpro" }}"
stream_name = "{{ getenv "LOGS_EXPORT_CLOUDWATCH_STREAM_NAME" "{{.app}}/{{.chan}}/{{.container_name}}" }}"
encoding.codec = "json"
encoding.timestamp_format = "rfc3339"
{{else if eq "stdout" (getenv "LOGS_EXPORT_TARGET") }}
[sinks.out_docker]
type = "console"
inputs = ["all"]
encoding.codec = "json"
encoding.timestamp_format = "rfc3339"
target = "stdout"
{{end}}

{{if conv.ToBool (getenv "METRICS_ENABLED" "false")}}
[sinks.metrics_prometheus_exporter]
type = "prometheus_exporter"
inputs = [
  {{if conv.ToBool (getenv "METRICS_NGINX_ENABLED" "true")}}
    "nginx_metrics",
  {{end}}
  {{if conv.ToBool (getenv "METRICS_PHP_FPM_ENABLED" "true")}}
    "php_fpm_metrics",
  {{end}}
]
flush_period_secs = 300
{{if ne "" (getenv "METRICS_AUTH_BEARER_TOKEN")}}
auth.strategy = "bearer"
auth.token = "{{ getenv "METRICS_AUTH_BEARER_TOKEN" }}"
{{end}}
{{end}}
